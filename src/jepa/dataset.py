import torch
from torch.nn import functional as F
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision import transforms
from typing import Optional
from copy import deepcopy
import os
import json
from .logger import WandbLogger
from .utils import set_seed
from .constants import PROJECT, ENTITY


# class DatasetWrapper(Dataset):
#     """
#     Wrapper class for a torchvision dataset.
#     Needed to have a common interface for all datasets.
#     """
#     def __init__(self, dataset: Dataset, jepa: bool = False):
#         """
#         :param dataset: a torch Dataset that spits pairs (x, y)
#         """
#         super().__init__()
#         self.dataset = dataset
#         self.jepa = jepa
    
#     def __len__(self):
#         return len(self.dataset)
    
#     def __getitem__(self, idx):
#         x, y = self.dataset[idx]
#         x_hat = deepcopy(x) if self.jepa else None
#         return {"x": x, "y": y, "x_hat": x_hat}


class JepaDataset(Dataset):
    """
    Simple dataset class for JEPA.
    """
    def __init__(self, data: torch.Tensor, labels: Optional[torch.Tensor] = None):
        """
        :param data: tensor of data points. Shape [P, ...]
        """
        super().__init__()
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        x_hat = x  # !! this is not a copy, but a reference
        y = self.labels[idx] if self.labels is not None else None
        return {"x": x, "x_hat": x_hat, "y": y}


class SimpleDataset(Dataset):
    """
    Simple dataset class for autoencoders.
    """
    def __init__(self, data: torch.Tensor, labels: Optional[torch.Tensor] = None):
        """
        :param data: tensor of data points. Shape [P, ...]
        """
        super().__init__()
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        x = self.data[idx]
        y = self.labels[idx] if self.labels is not None else None
        return {"x": x, "y": y}


class HiddenManifold:
    """
    This class is a collection of useful methods to generate, save and load
    datasets generated from a hidden manifold model.
    The hidden manifold model is a mathematically tractable model of
    data lying on a low-dimensional manifold embedded in a high-dimensional
    space. Data points are generated by sampling random patterns in a latent space
    and projecting them into a larger space through a nonlinear function
    (projection matrix + componentwise nonlinearity).
    """
    def __init__(self, save_dir: str = ""):
        self.save_dir = save_dir
        self.config = None

    @staticmethod
    def get_default_config() -> dict:
        """
        Return the default parameters to use for data generation.
        Here is a list of the parameters:
        - feature_distribution: distribution of the features
        - nonlinearity: nonlinearity applied after projection matrix
        - D: dimension of the latent space
        - N: dimension of the ambient space
        - P: number of data points to generate
        - p: probability of a pattern being active
        - noise: standard deviation of gaussian noise to add to data
        - device: device to use for data generation (cpu or cuda)
        """
        config = {
            "feature_distribution": "gaussian",
            "nonlinearity": "tanh",
            "D": 64,
            "N": 1024,
            "P": 8192,
            "p": 0.5,
            "noise": 0.0,
            "device": "cuda" if torch.cuda.is_available() else "cpu",
        }
        return config
    
    @staticmethod
    def build_config(**kwargs) -> dict:
        """
        Build a configuration dictionary from default parameters.
        Override default parameters with kwargs.
        """
        config = HiddenManifold.get_default_config()
        for key, value in kwargs.items():
            if key not in config:
                raise ValueError(f"Unknown parameter {key}")
            config[key] = value
        return config

    def get_config(self) -> dict:
        """
        Return the config used for the last data generation.
        """
        return self.config
    
    def generate_dataset(self, config: Optional[dict] = None) -> torch.Tensor:
        """
        Generate a random feature dataset using parameters
        specified in config. Expected config parameters are listed
        in get_default_config().
        In a random feature model, data points are the sum of a 
        random subset of a given set of features.
        """
        if config is None:
            config = HiddenManifold.get_default_config()
        self.config = config
        match config["feature_distribution"]:
            case "gaussian":
                feature_generator = torch.randn
            case _:
                raise NotImplementedError
        match config["nonlinearity"]:
            case "relu":
                sigma = F.relu
            case "tanh":
                sigma = F.tanh
            case _:
                raise NotImplementedError
        D, N, P = config["D"], config["N"], config["P"]
        p, noise, device = config["p"], config["noise"], config["device"]

        feature_matrix = feature_generator((D, N))
        latent_patterns = torch.bernoulli(p * torch.ones((P, D)))
        data = sigma(torch.matmul(latent_patterns, feature_matrix) / torch.sqrt(torch.tensor(D)))
        data += noise * torch.randn((P, N))
        return data.to(device)
    
    @staticmethod
    def get_dirname(id: str) -> str:
        """
        Return the name of the directory where to save a dataset with a given id.
        """
        return f"rf_{id}"
    
    def get_filepath(self, id: str) -> str:
        """
        Return the full path to a dataset with a given id.
        """
        filename = self.get_filename(id)
        save_dir = self.save_dir
        return f"{save_dir}/{filename}" if save_dir else filename

    def save_dataset(
            self,
            dataset: torch.Tensor,
            id: str,
            exist_ok: bool = False,
            log_to_wandb: bool = True
        ) -> None:
        """
        Save generated dataset to a file. Important for
        reproducibility of results.
        Place the file in a directory, together with a json file
        with the parameters used for data generation.
        :param dataset: dataset to save
        :param id: unique identifier for the dataset
        :param exist_ok: if False, raise an error if the directory already exists
        :param log_to_wandb: if True, log the dataset to Weights and Biases.
        """
        dataset_dir = os.path.join(self.save_dir, self.get_dirname(id))
        os.makedirs(dataset_dir, exist_ok=exist_ok)
        print(f"Saving dataset in directory {dataset_dir}")
        filepath = os.path.join(dataset_dir, f"dataset.pt")
        torch.save(dataset, filepath)
        metadata = self.get_config()
        metadata["id"] = id
        metadata["dataset_path"] = filepath
        metadata["dataset_dir"] = dataset_dir
        del metadata["device"]
        with open(os.path.join(dataset_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f)
        if log_to_wandb:
            WandbLogger.log_dataset(dataset, metadata, project=PROJECT, entity=ENTITY)

    def load_dataset(self, id: str) -> tuple[torch.Tensor, dict]:
        """
        Load a dataset saved with saved_dataset() in memory.
        """
        dataset_dir = os.path.join(self.save_dir, self.get_dirname(id))
        metadata_path = os.path.join(dataset_dir, "metadata.json")
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
        dataset_path = metadata["dataset_path"]
        dataset = torch.load(dataset_path)
        return dataset, metadata


def load_mnist(
    train: bool = True,
    log_to_wandb: bool = False,
    root: str = "data",
    project: str = PROJECT,
    shuffle: Optional[int] = None,
    jepa: bool = False,
    num_samples: Optional[int] = None,
) -> tuple[Dataset, dict]:
    """
    Load MNIST dataset, flatten digits and scale pixel intensities to [-1, 1].
    Return it as a Dataset, together with metadata.
    :param log_to_wandb: if True, log the dataset to Weights and Biases (before shuffling).
    :param shuffle: seed for shuffling the dataset. If None, don't shuffle.
    :param jepa: if True, the returned dataset will also spit x_hat for jepa training.
    :param num_samples: if not None, return only the first num_samples samples after shuffling.
    """
    dataset = datasets.MNIST(
        root=root,
        train=train,
        download=True,
    )
    data, targets = dataset.data, dataset.targets
    data = 2 * (data / 255) - 1
    data = data.flatten(start_dim=1)
    split = "train" if train else "test"
    metadata = {"id": f"mnist-{split}", "shuffle": shuffle, "dataset_dir": "data/MNIST", "num_samples": num_samples}
    if log_to_wandb:
        # TODO: this should also log labels
        filepath = os.path.join(metadata["dataset_dir"], metadata["id"])
        torch.save(data, filepath)
        WandbLogger.log_dataset(data, metadata, project=project, entity=ENTITY)
    if shuffle is not None:
        set_seed(shuffle)
        perm = torch.randperm(len(data))
        data = data[perm]
        targets = targets[perm]
    if num_samples is not None:
        assert num_samples <= len(data), "num_samples must be less than the dataset size"
        data = data[:num_samples]
        targets = targets[:num_samples]
    dataset_class = JepaDataset if jepa else SimpleDataset
    dataset = dataset_class(data, targets)
    return dataset, metadata


def load_cifar(
    train: bool = True,
    log_to_wandb: bool = False,
    root: str = "data",
    project: str = PROJECT,
    num_classes: int = 10,
    shuffle: Optional[int] = None,
    jepa: bool = False,
    num_samples: Optional[int] = None,
) -> tuple[torch.Tensor, dict]:
    """
    Load CIFAR10 or CIFAR100 dataset, flatten images and scale pixel intensities to [-1, 1].
    Return it as a Dataset, together with metadata.
    :param log_to_wandb: if True, log the dataset to Weights and Biases (before shuffling).
    :param shuffle: seed for shuffling the dataset. If None, don't shuffle.
    :param jepa: if True, the returned dataset will also spit x_hat for jepa training.
    :param num_samples: if not None, return only the first num_samples samples after shuffling.
    """
    cifar = datasets.CIFAR10 if num_classes == 10 else datasets.CIFAR100
    dataset = cifar(
        root=root,
        train=train,
        download=True,
    )
    data, targets = dataset.data, dataset.targets
    data = 2 * (data / 255) - 1
    data = data.flatten(start_dim=1)
    split = "train" if train else "test"
    metadata = {"id": f"cifar{num_classes}-{split}", "dataset_dir": f"data/cifar-{num_classes}-batches-py", "num_samples": num_samples, "shuffle": shuffle}
    if log_to_wandb:
        filepath = os.path.join(metadata["dataset_dir"], metadata["id"])
        os.makedirs(metadata["dataset_dir"], exist_ok=True)
        print(f"Saving dataset in {filepath}")
        torch.save(data, filepath)
        print(f"Logging dataset {metadata['id']} to wandb project {project}.")
        WandbLogger.log_dataset(data, metadata, project=project, entity=ENTITY)
    if shuffle is not None:
        set_seed(shuffle)
        perm = torch.randperm(len(data))
        data = data[perm]
        targets = targets[perm]
    if num_samples is not None:
        assert num_samples <= len(data), "num_samples must be less than the dataset size"
        data = data[:num_samples]
        targets = targets[:num_samples]
    dataset_class = JepaDataset if jepa else SimpleDataset
    dataset = dataset_class(data, targets)
    return dataset, metadata
